[2023-09-06 22:00:35,288] ERROR [networking.node.Net_Node._T]: Message could not be encoded:
ID: _T_4; TO: rig_3; SENDER: _T; KEY: START; FLAGS: {'NOREPEAT': True}; VALUE: {'session_info': {'subject_name': 'XXX', 'subject_weight': 10.0, 'rig_id': 'rig_3', 'protocol': 'random_dot_motion', 'experiment': 'rt_dynamic_training'}, 'session_config': 'from pathlib import Path\n\nimport numpy as np\nfrom scipy.stats import pearson3\n\nfrom protocols.random_dot_motion.core.config import config\n\nREQUIRED_HARDWARE = config.REQUIRED_HARDWARE\n\nREQUIRED_MODULES = config.REQUIRED_MODULES\n\nTASK = config.TASK\n\nSTIMULUS = config.STIMULUS\n\nDATAFILES = config.DATAFILES\n\nSUBJECT = config.SUBJECT\n\n\nTASK["epochs"]["stimulus"]["passive_viewing"] = lambda coh_level: pearson3.rvs(skew=0.6, loc=(coh_level - 1) * 10, scale=1.5, size=1)[0]\n\nTASK["epochs"]["reinforcement"] = (\n    {\n        "tag": "Reinforcement epoch. Returns delay in stimulus display and delay screen duration (usually white).",\n        "durations": {\n            "correct": lambda response_time: {\n                "duration": 0.300,\n                "delay": 0.000,\n            },\n            "incorrect": lambda response_time: {\n                "duration": 1.000,\n                "delay": 8 * (np.exp(-2 * response_time)),\n            },\n            "invalid": lambda response_time: {\n                "duration": 1.000,\n                "delay": 8 * (np.exp(-2 * response_time)),\n            },\n        },\n    },\n)\n\nGRADUATION = {\n    "direction": {\n        "tag": "Direction of graduation. 0: \'forward\' or 1:\'forward and backward\'",\n        "value": 1,\n    },\n    "coherence_levels": {\n        "tag": "List of all coherence levels and their properties used in this phase",\n        "value": {\n            1: np.array([-100, 100]),\n            2: np.array([-100, -72, 72, 100]),\n            3: np.array([-100, -72, -36, 36, 72, 100]),\n            4: np.array([-100, -72, -36, -18, 18, 36, 72, 100]),\n            5: np.array([-100, -72, -36, -18, -9, 9, 18, 36, 72, 100]),\n        },\n    },\n    "accuracy": {\n        "rolling_widows": {\n            "tag": "Number of trials per coherence to consider for accuracy calculation",\n            "value": 50,\n        },\n        "thresholds": {\n            "tag": "List of all accuracy conditions for each coherence level to move forward (or backward)",\n            "value": {\n                1: np.array([0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7]),\n                2: np.array([0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7]),\n                3: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n                4: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n                5: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n                6: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n            },\n        },\n    },\n    "trials_threshold": {\n        "tag": "Number of trials required to move forward (or backward) for each coherence level",\n        "value": {\n            1: 0,\n            2: 0,\n            3: 0,\n            4: 200,\n            5: 200,\n            6: 200,\n        },\n    },\n    "reward_change": {\n        "tag": "Reward change for each coherence level increase (or decrease)",\n        "value": {\n            "increase": 0.3,\n            "decrease": 0.3,\n        },\n    },\n}\n\n\ndef grad_check(current_level, accuracy, level_change_trial_counter):\n    graduation_direction = GRADUATION["direction"]["value"]\n    accuracy_thesholds = GRADUATION["accuracy"]["thresholds"]["value"]\n    trials_threshold = GRADUATION["trials_threshold"]["value"]\n\n    next_coherence_level = current_level\n    new_trial_counter = level_change_trial_counter + 1\n\n    # forward graduation\n    while next_coherence_level < 5:\n        if all(accuracy >= accuracy_thesholds[next_coherence_level]) and (new_trial_counter >= trials_threshold[next_coherence_level]):\n            next_coherence_level = next_coherence_level + 1\n            new_trial_counter = 0\n        else:\n            break\n\n    # backward graduation\n    if graduation_direction == 0:\n        while next_coherence_level > 1:\n            if any(accuracy < accuracy_thesholds[next_coherence_level - 1]):\n                next_coherence_level = next_coherence_level - 1\n                new_trial_counter = 0\n            else:\n                break\n\n    print(f"Coherence level changed from {current_level} to {next_coherence_level}")\n    print(f"Trial counter: {level_change_trial_counter} to {new_trial_counter}")\n    return next_coherence_level\n\n\nif __name__ == "__main__":\n    GRADUATION["direction"]["value"] = 0\n    accuracy = np.array([0.7, 0.2, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7])\n    current_level = 5\n    level_change_trial_counter = 1\n    current_level = grad_check(current_level, accuracy, level_change_trial_counter)\n    level_change_trial_counter = 201\n    current_level = grad_check(current_level, accuracy, level_change_trial_counter)\n', 'subject_config': {'name': 'XXX', 'base_weight': 10.0, 'start_weight': 10.0, 'prct_weight': 100.0, 'protocol': 'random_dot_motion', 'experiment': 'rt_dynamic_training', 'session': '1_1', 'session_uuid': '859912dc-faa2-4c75-a71f-a7b05ee7aaec', 'rolling_perf': {'rolling_window': 50, 'history': {-100: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -72: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -36: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -18: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -9: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 0: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 9: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 18: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 36: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 72: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 100: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 'history_indices': {-100: 0, -72: 0, -36: 0, -18: 0, -9: 0, 0: 0, 9: 0, 18: 0, 36: 0, 72: 0, 100: 0}, 'accuracy': {-100: 0, -72: 0, -36: 0, -18: 0, -9: 0, 0: 0, 9: 0, 18: 0, 36: 0, 72: 0, 100: 0}, 'current_coherence_level': 2, 'trials_in_current_level': 0, 'total_attempts': 0, 'total_reward': 0, 'reward_volume': 3}}}
[2023-09-06 22:23:54,958] ERROR [networking.node.Net_Node._T]: Message could not be encoded:
ID: _T_4; TO: rig_3; SENDER: _T; KEY: START; FLAGS: {'NOREPEAT': True}; VALUE: {'session_info': {'subject_name': 'XXX', 'subject_weight': 25.0, 'rig_id': 'rig_3', 'protocol': 'random_dot_motion', 'experiment': 'rt_dynamic_training'}, 'session_config': 'from pathlib import Path\n\nimport numpy as np\nfrom scipy.stats import pearson3\n\nfrom protocols.random_dot_motion.core.config import config\n\nREQUIRED_HARDWARE = config.REQUIRED_HARDWARE\n\nREQUIRED_MODULES = config.REQUIRED_MODULES\n\nTASK = config.TASK\n\nSTIMULUS = config.STIMULUS\n\nDATAFILES = config.DATAFILES\n\nSUBJECT = config.SUBJECT\n\n\nTASK["epochs"]["stimulus"]["passive_viewing"] = lambda coh_level: pearson3.rvs(skew=0.6, loc=(coh_level - 1) * 10, scale=1.5, size=1)[0]\n\nTASK["epochs"]["reinforcement"] = (\n    {\n        "tag": "Reinforcement epoch. Returns delay in stimulus display and delay screen duration (usually white).",\n        "durations": {\n            "correct": lambda response_time: {\n                "duration": 0.300,\n                "delay": 0.000,\n            },\n            "incorrect": lambda response_time: {\n                "duration": 1.000,\n                "delay": 8 * (np.exp(-2 * response_time)),\n            },\n            "invalid": lambda response_time: {\n                "duration": 1.000,\n                "delay": 8 * (np.exp(-2 * response_time)),\n            },\n        },\n    },\n)\n\nGRADUATION = {\n    "direction": {\n        "tag": "Direction of graduation. 0: \'forward\' or 1:\'forward and backward\'",\n        "value": 1,\n    },\n    "coherence_levels": {\n        "tag": "List of all coherence levels and their properties used in this phase",\n        "value": {\n            1: np.array([-100, 100]),\n            2: np.array([-100, -72, 72, 100]),\n            3: np.array([-100, -72, -36, 36, 72, 100]),\n            4: np.array([-100, -72, -36, -18, 18, 36, 72, 100]),\n            5: np.array([-100, -72, -36, -18, -9, 9, 18, 36, 72, 100]),\n        },\n    },\n    "accuracy": {\n        "rolling_widows": {\n            "tag": "Number of trials per coherence to consider for accuracy calculation",\n            "value": 50,\n        },\n        "thresholds": {\n            "tag": "List of all accuracy conditions for each coherence level to move forward (or backward)",\n            "value": {\n                1: np.array([0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7]),\n                2: np.array([0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7]),\n                3: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n                4: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n                5: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n                6: np.array([0.7, 0.7, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7]),\n            },\n        },\n    },\n    "trials_threshold": {\n        "tag": "Number of trials required to move forward (or backward) for each coherence level",\n        "value": {\n            1: 0,\n            2: 0,\n            3: 0,\n            4: 200,\n            5: 200,\n            6: 200,\n        },\n    },\n    "reward_change": {\n        "tag": "Reward change for each coherence level increase (or decrease)",\n        "value": {\n            "increase": 0.3,\n            "decrease": 0.3,\n        },\n    },\n}\n\n\ndef grad_check(current_level, accuracy, level_change_trial_counter):\n    graduation_direction = GRADUATION["direction"]["value"]\n    accuracy_thesholds = GRADUATION["accuracy"]["thresholds"]["value"]\n    trials_threshold = GRADUATION["trials_threshold"]["value"]\n\n    next_coherence_level = current_level\n    new_trial_counter = level_change_trial_counter + 1\n\n    # forward graduation\n    while next_coherence_level < 5:\n        if all(accuracy >= accuracy_thesholds[next_coherence_level]) and (new_trial_counter >= trials_threshold[next_coherence_level]):\n            next_coherence_level = next_coherence_level + 1\n            new_trial_counter = 0\n        else:\n            break\n\n    # backward graduation\n    if graduation_direction == 0:\n        while next_coherence_level > 1:\n            if any(accuracy < accuracy_thesholds[next_coherence_level - 1]):\n                next_coherence_level = next_coherence_level - 1\n                new_trial_counter = 0\n            else:\n                break\n\n    print(f"Coherence level changed from {current_level} to {next_coherence_level}")\n    print(f"Trial counter: {level_change_trial_counter} to {new_trial_counter}")\n    return next_coherence_level\n\n\nif __name__ == "__main__":\n    GRADUATION["direction"]["value"] = 0\n    accuracy = np.array([0.7, 0.2, 0.7, 0.0, 0.0, 0.0, 0.0, 0.7, 0.7, 0.7])\n    current_level = 5\n    level_change_trial_counter = 1\n    current_level = grad_check(current_level, accuracy, level_change_trial_counter)\n    level_change_trial_counter = 201\n    current_level = grad_check(current_level, accuracy, level_change_trial_counter)\n', 'subject_config': {'name': 'XXX', 'base_weight': 25.0, 'start_weight': 25.0, 'prct_weight': 100.0, 'protocol': 'random_dot_motion', 'experiment': 'rt_dynamic_training', 'session': '1_1', 'session_uuid': '1cd9804b-8ef7-449b-bc87-de39ce5560a2', 'rolling_perf': {'rolling_window': 50, 'history': {-100: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -72: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -36: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -18: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -9: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 0: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 9: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 18: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 36: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 72: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 100: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 'history_indices': {-100: 0, -72: 0, -36: 0, -18: 0, -9: 0, 0: 0, 9: 0, 18: 0, 36: 0, 72: 0, 100: 0}, 'accuracy': {-100: 0, -72: 0, -36: 0, -18: 0, -9: 0, 0: 0, 9: 0, 18: 0, 36: 0, 72: 0, 100: 0}, 'current_coherence_level': 2, 'trials_in_current_level': 0, 'total_attempts': 0, 'total_reward': 0, 'reward_volume': 3}}}
